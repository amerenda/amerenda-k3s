---
# k3s Controller Setup
# This playbook sets up a k3s controller node with etcd
# Run with: ansible-playbook -i inventory.ini k3s-controller.yml

- name: Setup k3s Controller Node
  hosts: controllers
  become: yes
  gather_facts: yes
  serial: [1, "{{ groups['controllers'] | length - 1 }}"]  # Run first controller alone, then rest in parallel
  tags: [k3s, etcd, controllers, install]

  vars:
    # k3s configuration
    k3s_version: "v1.33.5+k3s1"
    k3s_data_dir: "/var/lib/rancher/k3s"
    etcd_data_dir: "/var/lib/rancher/k3s/server/db"
    
    # ArgoCD configuration
    argocd_version: "8.6.0"
    argocd_namespace: "default"
    argocd_release_name: "argocd"
    
    # Local user configuration
    local_user: "{{ ansible_user | default('alex') }}"
    
    # Network configuration
    cluster_cidr: "10.42.0.0/16"
    service_cidr: "10.43.0.0/16"
    cluster_dns: "10.43.0.10"
    
    # Controller configuration
    controller_ips: "{{ groups['controllers'] | map('extract', hostvars, 'ansible_host') | list }}"
    first_controller: "{{ groups['controllers'][0] }}"
    is_first_controller: "{{ inventory_hostname == first_controller }}"
    
    # Cluster replacement protection (defaults to false if not set)
    replace_cluster_bool: "{{ replace_cluster | default(false) | bool }}"
    
    # Longhorn storage configuration
    enable_longhorn_storage: "{{ (longhorn_storage_enabled | default(true)) | bool }}"

    # etcd packages for controller nodes (K3s includes etcd binary, no separate packages needed)
    etcd_packages: []

  pre_tasks:
    - name: Check if SSH private key exists
      stat:
        path: "{{ ansible_ssh_private_key_file | default('~/.ssh/alex_id_ed25519') | expanduser }}"
      register: ssh_key_check
      delegate_to: localhost
      become: no
      run_once: true

    - name: Fail if SSH private key is missing
      fail:
        msg: "SSH private key not found at {{ ansible_ssh_private_key_file | default('~/.ssh/alex_id_ed25519') | expanduser }}. Please ensure the key exists before running this playbook."
      when: not ssh_key_check.stat.exists

    - name: Display SSH key information
      debug:
        msg: "Using SSH key: {{ ansible_ssh_private_key_file | default('~/.ssh/alex_id_ed25519') | expanduser }}"
      when: ssh_key_check.stat.exists

    - name: Check if etcd data directory exists
      stat:
        path: "{{ etcd_data_dir }}"
      register: etcd_exists

    - name: Check if k3s is already running
      systemd:
        name: k3s
      register: k3s_running
      ignore_errors: yes

    - name: Display cluster protection warning
      debug:
        msg: |
          ⚠️  WARNING: Existing k3s cluster detected! ⚠️
          ========================================
          etcd data directory exists: {{ etcd_exists.stat.exists }}
          k3s service status: {{ 'running' if k3s_running.status.ActiveState == 'active' else 'stopped' }}
          ========================================
          To replace existing cluster, use: -e replace_cluster=true
          This will DESTROY existing cluster data!
          ========================================
      when: etcd_exists.stat.exists and not (replace_cluster_bool)

    - name: Display replacement warning
      debug:
        msg: |
          ⚠️  WARNING: REPLACING EXISTING K3S CLUSTER! ⚠️
          ========================================
          This will DESTROY all existing cluster data!
          Make sure you have backups before proceeding!
          ========================================
      when: replace_cluster_bool

    - name: Skip installation if cluster exists and replacement not requested
      debug:
        msg: "Existing k3s cluster detected. Skipping installation. Use -e replace_cluster=true to replace."
      when: etcd_exists.stat.exists and not (replace_cluster_bool)
      tags: [k3s, protection]

    - name: Set k3s token from environment variable
      set_fact:
        k3s_token: "{{ ansible_env.K3S_TOKEN }}"
      when: ansible_env.K3S_TOKEN is defined and ansible_env.K3S_TOKEN != ""

    - name: Validate k3s token is set (only required when replacing)
      fail:
        msg: "k3s_token must be set when replace_cluster=true. Use -e k3s_token=your-token, set K3S_TOKEN env var, or set it in group_vars"
      when: replace_cluster_bool and (k3s_token is not defined or k3s_token == "")

    - name: Check if k3s token file exists on first controller
      stat:
        path: /var/lib/rancher/k3s/server/token
      register: existing_token_file
      when: is_first_controller
      tags: [validation]

    - name: Get existing cluster token from first controller
      shell: cat /var/lib/rancher/k3s/server/token
      register: existing_token
      when: is_first_controller and existing_token_file.stat.exists
      changed_when: false
      tags: [validation]

    - name: Validate token matches existing cluster (first controller)
      fail:
        msg: |
          ========================================
          ❌ TOKEN MISMATCH DETECTED! ❌
          ========================================
          The provided k3s_token does not match the existing cluster token.
          
          Current cluster token: {{ existing_token.stdout }}
          Provided token: {{ k3s_token }}
          
          To fix this, get the correct token from the first controller:
          ssh {{ ansible_host }} "sudo cat /var/lib/rancher/k3s/server/token"
          
          Then re-run the playbook with the correct token:
          ansible-playbook -i inventory.ini k3s-controller.yml -e k3s_token=CORRECT_TOKEN
          ========================================
      when: 
        - is_first_controller
        - existing_token_file.stat.exists
        - existing_token.stdout != k3s_token
      tags: [validation]

    - name: Display cluster configuration
      debug:
        msg: |
          ========================================
          k3s Controller Configuration
          ========================================
          Host: {{ inventory_hostname }} ({{ ansible_host }})
          Role: {{ 'First Controller' if is_first_controller else 'Additional Controller' }}
          k3s Version: {{ k3s_version }}
          etcd Data Dir: {{ etcd_data_dir }}
          Controller IPs: {{ controller_ips | join(', ') }}
          ========================================

  tasks:
    - name: Install etcd packages for controller nodes
      apt:
        name: "{{ etcd_packages }}"
        state: present
        update_cache: yes
      when: etcd_packages | length > 0
      tags: [etcd, packages]

    - name: Skip etcd package installation (K3s includes etcd binary)
      debug:
        msg: "Skipping etcd package installation - K3s includes its own etcd binary"
      when: etcd_packages | length == 0
      tags: [etcd, packages]

    - name: Resize root filesystem to use all available space
      shell: |
        echo "Resizing filesystem to use all available space..."
        
        # Find the root filesystem device
        ROOT_DEV=$(df / | tail -1 | awk '{print $1}')
        echo "Root device: $ROOT_DEV"
        
        # Extract the disk and partition number using sed instead of bash regex
        DISK=$(echo $ROOT_DEV | sed 's/p[0-9]*$//')
        PART_NUM=$(echo $ROOT_DEV | sed 's/.*p//')
        echo "Disk: $DISK, Partition: $PART_NUM"
        
        # Resize the partition to use all available space
        parted $DISK resizepart $PART_NUM 100%
        
        # Resize the filesystem to fill the partition
        resize2fs $ROOT_DEV
        
        echo "Filesystem resize complete"
      register: resize_result
      changed_when: true
      when: resize_rootfs | default(false)
      tags: [filesystem, resize]

    - name: Show filesystem resize result
      debug:
        var: resize_result.stdout_lines
      when: resize_result.stdout_lines is defined and (resize_rootfs | default(false))
      tags: [filesystem, resize]

    - name: Check available disk space after resize
      shell: df -h /
      register: disk_usage
      changed_when: false
      when: resize_rootfs | default(false)
      tags: [filesystem, resize]

    - name: Create k3s data directories
      file:
        path: "{{ item }}"
        state: directory
        owner: root
        group: root
        mode: '0755'
      loop:
        - "{{ k3s_data_dir }}"
        - "{{ etcd_data_dir }}"
        - "/etc/rancher/k3s"
      tags: [k3s, directories]

    - name: Check if k3s is already installed
      stat:
        path: /usr/local/bin/k3s
      register: k3s_installed
      tags: [k3s, check]

    - name: Uninstall existing k3s (when replacing cluster)
      shell: |
        # Use official k3s uninstall script
        /usr/local/bin/k3s-uninstall.sh || true
        
        # Also remove any remaining k3s-agent if it exists
        /usr/local/bin/k3s-agent-uninstall.sh || true
        
        echo "k3s completely uninstalled using official scripts"
      when: replace_cluster_bool and k3s_installed.stat.exists
      tags: [k3s, uninstall]

    - name: Download and install k3s (first controller - cluster init)
      shell: |
        curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="{{ k3s_version }}" INSTALL_K3S_EXEC="--cluster-init --token {{ k3s_token | default(ansible_env.K3S_TOKEN) }}" sh -
      when: (not k3s_installed.stat.exists or (replace_cluster_bool)) and is_first_controller and not (etcd_exists.stat.exists and not replace_cluster_bool)
      tags: [k3s, install]

    - name: Wait for first controller to be ready (for additional controllers)
      wait_for:
        host: "{{ hostvars[first_controller]['ansible_host'] }}"
        port: 6443
        timeout: 300
        delay: 10
      when: not is_first_controller
      tags: [k3s, wait]

    - name: Download and install k3s (additional controllers - join cluster)
      shell: |
        curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="{{ k3s_version }}" INSTALL_K3S_EXEC="--server https://{{ hostvars[first_controller]['ansible_host'] }}:6443 --token {{ k3s_token | default(ansible_env.K3S_TOKEN) }}" sh -
      when: (not k3s_installed.stat.exists or (replace_cluster_bool)) and not is_first_controller and not (etcd_exists.stat.exists and not replace_cluster_bool)
      ignore_errors: yes
      register: k3s_install_result
      tags: [k3s, install]

    - name: Deploy k3s systemd service
      template:
        src: k3s.service.j2
        dest: /etc/systemd/system/k3s.service
        owner: root
        group: root
        mode: '0644'
        backup: yes
      when: not (etcd_exists.stat.exists and not replace_cluster_bool) and (not k3s_installed.stat.exists or replace_cluster_bool)
      tags: [k3s, systemd]

    - name: Stop k3s service if running
      systemd:
        name: k3s
        state: stopped
      when: k3s_installed.stat.exists and replace_cluster_bool
      tags: [k3s, stop]

    - name: Configure k3s server for first controller
      template:
        src: k3s-server-config.yaml.j2
        dest: /etc/rancher/k3s/config.yaml
        owner: root
        group: root
        mode: '0644'
      when: is_first_controller
      tags: [k3s, config, first]

    - name: Configure k3s server for additional controllers
      template:
        src: k3s-server-config.yaml.j2
        dest: /etc/rancher/k3s/config.yaml
        owner: root
        group: root
        mode: '0644'
      when: not is_first_controller
      tags: [k3s, config, additional]

    - name: Reload systemd daemon
      systemd:
        daemon_reload: yes
      when: not (etcd_exists.stat.exists and not replace_cluster_bool) and (not k3s_installed.stat.exists or replace_cluster_bool)
      tags: [k3s, systemd]

    - name: Start and enable k3s service
      systemd:
        name: k3s
        state: started
        enabled: yes
      ignore_errors: yes
      register: k3s_start_result
      when: not (etcd_exists.stat.exists and not replace_cluster_bool)
      tags: [k3s, service]

    - name: Wait for k3s API to be ready
      shell: |
        curl -k -s -o /dev/null -w "%{http_code}" https://{{ ansible_host }}:6443/healthz
      register: k3s_health_check
      until: k3s_health_check.rc == 0 and (k3s_health_check.stdout == "200" or k3s_health_check.stdout == "401")
      retries: 30
      delay: 5
      tags: [k3s, wait]

    - name: Create .kube directory if it doesn't exist
      file:
        path: "/home/{{ local_user }}/.kube"
        state: directory
        mode: '0755'
      when: is_first_controller
      delegate_to: localhost
      become: no
      remote_user: "{{ local_user }}"
      tags: [k3s, kubeconfig]

    - name: Create HA kubeconfig with all controller endpoints
      shell: |
        # Get the original kubeconfig from the first controller
        kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml config view --raw > /tmp/k3s-config.yaml
        
        # Extract certificate and key data
        CERT_DATA=$(kubectl --kubeconfig=/tmp/k3s-config.yaml config view --raw -o jsonpath='{.clusters[0].cluster.certificate-authority-data}')
        CLIENT_CERT=$(kubectl --kubeconfig=/tmp/k3s-config.yaml config view --raw -o jsonpath='{.users[0].user.client-certificate-data}')
        CLIENT_KEY=$(kubectl --kubeconfig=/tmp/k3s-config.yaml config view --raw -o jsonpath='{.users[0].user.client-key-data}')
        
        # Create HA kubeconfig with all controller endpoints
        cat > "/home/{{ local_user }}/.kube/config" << EOF
        apiVersion: v1
        kind: Config
        clusters:
        - cluster:
            certificate-authority-data: ${CERT_DATA}
            server: https://{{ controller_ips | join(':6443,https://') }}:6443
          name: k3s-cluster
        contexts:
        - context:
            cluster: k3s-cluster
            user: k3s-user
          name: k3s-context
        current-context: k3s-context
        users:
        - name: k3s-user
          user:
            client-certificate-data: ${CLIENT_CERT}
            client-key-data: ${CLIENT_KEY}
        EOF
        
        # Clean up temp file
        rm -f /tmp/k3s-config.yaml
        
        # Set proper permissions
        chmod 600 "/home/{{ local_user }}/.kube/config"
        chown {{ local_user }}:{{ local_user }} "/home/{{ local_user }}/.kube/config"
      when: is_first_controller
      delegate_to: localhost
      become: no
      remote_user: "{{ local_user }}"
      tags: [k3s, kubeconfig]

    - name: Add ArgoCD Helm repository
      kubernetes.core.helm_repository:
        name: argo
        repo_url: https://argoproj.github.io/argo-helm
      when: is_first_controller
      delegate_to: localhost
      become: no
      remote_user: "{{ local_user }}"
      tags: [argocd, helm]

    - name: Install ArgoCD
      kubernetes.core.helm:
        name: "{{ argocd_release_name }}"
        chart_ref: argo/argo-cd
        release_namespace: "{{ argocd_namespace }}"
        create_namespace: false
        chart_version: "{{ argocd_version }}"
        kubeconfig: "/home/{{ local_user }}/.kube/config"
        values_files:
          - "{{ playbook_dir | dirname }}/bootstrap/argocd/values.yaml"
        wait: true
        timeout: 300s
      when: is_first_controller
      delegate_to: localhost
      become: no
      remote_user: "{{ local_user }}"
      tags: [argocd, helm]

    - name: Wait for ArgoCD to be ready
      shell: |
        kubectl --kubeconfig="/home/{{ local_user }}/.kube/config" get pods -n "{{ argocd_namespace }}" -l "app.kubernetes.io/name=argocd-server" -o jsonpath='{.items[0].status.phase}' | grep -q "Running"
      register: argocd_ready
      until: argocd_ready.rc == 0
      retries: 30
      delay: 10
      when: is_first_controller
      delegate_to: localhost
      become: no
      remote_user: "{{ local_user }}"
      tags: [argocd, wait]

    - name: Apply root-app.yaml to bootstrap GitOps
      shell: |
        kubectl --kubeconfig="/home/{{ local_user }}/.kube/config" apply -f "{{ playbook_dir | dirname }}/gitops/apps/root-app.yaml"
      when: is_first_controller
      delegate_to: localhost
      become: no
      remote_user: "{{ local_user }}"
      tags: [argocd, gitops, bootstrap]

    - name: Display ArgoCD installation reminder
      debug:
        msg: "ArgoCD installation complete! Remember to bootstrap Bitwarden credentials: kubectl apply -f bootstrap/bitwarden-credentials-secret.yaml"
      when: is_first_controller
      tags: [argocd, reminder]

    - name: Check if kubectl is available and cluster is ready
      shell: |
        kubectl get nodes --no-headers | wc -l
      delegate_to: localhost
      become: no
      register: kubectl_check
      failed_when: false
      when: enable_longhorn_storage
      tags: [k3s, labels, longhorn, storage-gate]

    - name: Add longhorn-storage label to all nodes
      shell: |
        kubectl label node {{ inventory_hostname }} longhorn-storage=true --overwrite
      delegate_to: localhost
      become: no
      when: enable_longhorn_storage and kubectl_check.rc == 0
      tags: [k3s, labels, longhorn, storage]

    - name: Skip Longhorn label (kubectl not ready)
      debug:
        msg: "Skipping Longhorn storage label - kubectl not ready or cluster not available"
      when: enable_longhorn_storage and kubectl_check.rc != 0
      tags: [k3s, labels, longhorn, storage]

    - name: Remove longhorn-storage label from all nodes
      shell: |
        kubectl label node {{ inventory_hostname }} longhorn-storage- || true
      delegate_to: localhost
      become: no
      when: not enable_longhorn_storage
      tags: [k3s, labels, longhorn, storage]

  handlers:
    - name: restart k3s
      systemd:
        name: k3s
        state: restarted
